{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "PATH_TEST = '/home/ffajri/Data/CDTB/TEST/*'\n",
    "PATH_DEV = '/home/ffajri/Data/CDTB/VALIDATE/*'\n",
    "PATH_TRAIN = '/home/ffajri/Data/CDTB/TRAIN/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_nuclear = {'LEFT': 'NS', 'RIGHT': 'SN', 'ALL': 'NN'}\n",
    "relationmap = {'因果类': ['因果关系', '推断关系', '假设关系', '目的关系', '条件关系', '背景关系'],\n",
    "               '并列类': ['并列关系', '顺承关系', '递进关系', '选择关系', '对比关系'],\n",
    "               '转折类': ['转折关系', '让步关系'],\n",
    "               '解说类': ['解说关系', '总分关系', '例证关系', '评价关系']}\n",
    "rev_relationmap = {}\n",
    "for coarse_class, fine_classes in relationmap.items():\n",
    "    rev_relationmap.update((sub_class, coarse_class) for sub_class in fine_classes)\n",
    "    \n",
    "def get_text(node):\n",
    "    if node.tag == 'TEXT':\n",
    "        return node.text\n",
    "    elif len(node) > 0:\n",
    "        text = ''\n",
    "        for n in node:\n",
    "            text += get_text(n)\n",
    "        return text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def extract(node):\n",
    "    assert node.tag == 'RELATION'\n",
    "    nuclearity = map_nuclear[node.attrib['NUCLEAR']]\n",
    "    relation = node.attrib.get('CTYPE', None)\n",
    "    if relation is None:  \n",
    "        relation = rev_relationmap[node.attrib['TYPE']]\n",
    "    connectives = node.attrib['CONNECTIVES']\n",
    "    node1 = get_text(node[0])\n",
    "    node2 = ''\n",
    "    for n in node[1:]:\n",
    "        node2 += get_text(n)\n",
    "    return [node1, node2, nuclearity, relation, connectives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(PATH):\n",
    "    all_data = []\n",
    "    for file in glob.glob(PATH):\n",
    "        tree = ET.parse(file)\n",
    "        root = list(tree.getroot())\n",
    "        for child in root:\n",
    "            data = []\n",
    "            connectives = {\"\":\"\"}\n",
    "            queue = [child]\n",
    "            while len(queue) > 0:\n",
    "                current = queue.pop(0)\n",
    "                if current.tag == 'RELATION':\n",
    "                    data.append(extract(current))\n",
    "                if current.tag == 'CONNECTIVE':\n",
    "                    connectives[current.attrib['ID']] = current.text\n",
    "                if len(current) > 0:\n",
    "                    for node in current:\n",
    "                        queue.append(node)\n",
    "            for idx in range(len(data)):\n",
    "                problem=False\n",
    "                for id in data[idx][4].split('-'):\n",
    "                    if connectives.get(id,None) is None:\n",
    "                        data[idx][4] = ''\n",
    "                        problem=True\n",
    "                        break\n",
    "                if not problem:\n",
    "                    ids = [connectives[id] for id in data[idx][4].split('-')]\n",
    "                    data[idx][4] = '-'.join(ids)\n",
    "            all_data += data\n",
    "    return all_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segmentation(PATH):\n",
    "    all_data = []\n",
    "    for file in glob.glob(PATH):\n",
    "        tree = ET.parse(file)\n",
    "        root = list(tree.getroot())\n",
    "        for child in root:\n",
    "            if child.tag != 'PARAGRAPH': continue\n",
    "            data = []\n",
    "            queue = [child]\n",
    "            while len(queue) > 0:\n",
    "                current = queue.pop(0)\n",
    "                if current.tag == 'TEXT':\n",
    "                    data.append(current.text)\n",
    "                if len(current) > 0:\n",
    "                    new_queue = []\n",
    "                    for node in current:\n",
    "                        new_queue.append(node)\n",
    "                    queue = new_queue+queue\n",
    "            if len(data)>1:\n",
    "                all_data.append(data)\n",
    "    return all_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = process(PATH_TEST)\n",
    "dev = process(PATH_DEV)\n",
    "train = process(PATH_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def write(fname, array):\n",
    "    df = pd.DataFrame()\n",
    "    edus1 = []; edus2=[]; nucs=[]; relations=[]\n",
    "    for edu1, edu2, nuc, relation, _ in array:\n",
    "        edus1.append(edu1)\n",
    "        edus2.append(edu2)\n",
    "        nucs.append(nuc)\n",
    "        relations.append(relation)\n",
    "    df['edu1']=edus1\n",
    "    df['edu2']=edus2\n",
    "    df['nuclear']=nucs\n",
    "    df['relation']=relations\n",
    "    print(fname, df.shape)\n",
    "    df.to_csv(fname, index=False)\n",
    "    \n",
    "def write_dissent(fname, array, dic=None):                \n",
    "    f = open(fname, 'w')\n",
    "    for edu1, edu2, _, _, connective in array:\n",
    "        if connective == '' or (dic is not None and connective not in dic):\n",
    "            continue\n",
    "        f.write(edu1+'\\t'+edu2+'\\t'+connective+'\\n')\n",
    "    f.close()\n",
    "    \n",
    "def write_segment(fname, array):\n",
    "    final_data={}\n",
    "    for idx, data in enumerate(array):\n",
    "        final_data[idx]=data\n",
    "    json.dump(final_data, open(fname, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ffajri/Workspace/WhatDiscourse/rst/data_chinese/train.csv (6159, 4)\n",
      "/home/ffajri/Workspace/WhatDiscourse/rst/data_chinese/dev.csv (353, 4)\n",
      "/home/ffajri/Workspace/WhatDiscourse/rst/data_chinese/test.csv (809, 4)\n"
     ]
    }
   ],
   "source": [
    "#NUCLEARITY and RELATION PREDICTION\n",
    "\n",
    "write('/home/ffajri/Workspace/discourse_probing/rst/data/data_zh/train.csv', train)\n",
    "write('/home/ffajri/Workspace/discourse_probing/rst/data/data_zh/dev.csv', dev)\n",
    "write('/home/ffajri/Workspace/discourse_probing/rst/data/data_zh/test.csv', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEGMENTATION\n",
    "\n",
    "test_seg = process_segmentation(PATH_TEST)\n",
    "dev_seg = process_segmentation(PATH_DEV)\n",
    "train_seg = process_segmentation(PATH_TRAIN)\n",
    "\n",
    "write_segment('/home/ffajri/Workspace/discourse_probing/segment/data/data_zh/test_edu.json', test_seg)\n",
    "write_segment('/home/ffajri/Workspace/discourse_probing/segment/data/data_zh/dev_edu.json', dev_seg)\n",
    "write_segment('/home/ffajri/Workspace/discourse_probing/segment/data/data_zh/train_edu.json', train_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCOURSE CONNECTIVE PREDICTION\n",
    "\n",
    "dic=set()\n",
    "for edu1, edu2, _, _, connective in train:\n",
    "    if connective != '': dic.add(connective)\n",
    "\n",
    "write_dissent('/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/train.tsv', train)\n",
    "write_dissent('/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/dev.tsv', dev, dic)\n",
    "write_dissent('/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/test.tsv', test, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refine dissent for Chinese\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def read(fname):\n",
    "    ret = []\n",
    "    for line in open(fname).readlines():\n",
    "        ret.append(line.split('\\t')[-1].strip())\n",
    "    return ret\n",
    "\n",
    "a = Counter(read('/home/ffajri/Workspace/WhatDiscourse/dissent/data/data_zh/train.tsv'))\n",
    "selected_connective = [word for word in a.keys() if a[word]>12]\n",
    "\n",
    "\n",
    "def refine(path, target_path):\n",
    "    new_data = []\n",
    "    for line in open(path).readlines():\n",
    "        text1, text2, conn = line.split('\\t')\n",
    "        conn = conn.strip()\n",
    "        if conn not in selected_connective:\n",
    "            conn = 'other'\n",
    "        new_data.append([text1, text2, conn])\n",
    "    f = open(target_path, 'w')\n",
    "    for text1, text2, conn in new_data:\n",
    "        f.write(text1+'\\t'+text2+'\\t'+conn+'\\n')\n",
    "    f.close()\n",
    "\n",
    "refine('/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/train.tsv','/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/train.tsv')\n",
    "refine('/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/dev.tsv','/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/dev.tsv')\n",
    "refine('/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/test.tsv','/home/ffajri/Workspace/discourse_probing/dissent/data/data_zh/test.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
